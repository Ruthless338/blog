---
title: 高斯核
date: 2025-07-09
categories:
    - 科研 
tags: 
    - 机器学习
    - 核函数
---



高斯核（Gaussian Kernel）是一种常用的核函数（Kernel Function），广泛应用于支持向量机（SVM）、核主成分分析（KPCA）、聚类等机器学习任务中，尤其擅长处理**非线性问题**。其核心原理是通过“隐式映射”将低维空间中线性不可分的数据转换到高维空间，从而在高维空间中实现线性可分，同时避免了直接计算高维空间的复杂运算。

<!--more-->


### 一、核函数的基本概念
在解释高斯核之前，先明确核函数的作用：  
现实中很多数据在低维空间是线性不可分的（比如二维平面上的环形数据），此时需要将数据映射到更高维的空间（比如三维），使得在高维空间中数据可以用线性超平面分隔。  

设低维空间的样本为$x, y$，映射函数$\phi$将其映射到高维空间$\phi(x), \phi(y)$，则高维空间中两个样本的内积为$\phi(x) \cdot \phi(y)$。  
核函数的定义为：  
$$ K(x, y) = \phi(x) \cdot \phi(y) $$  
即**核函数直接计算低维样本的某种关系，等价于高维空间中映射后的内积**。  

核函数的优势在于：无需显式定义映射函数$\phi$（甚至无需知道$\phi$的具体形式），就能间接实现高维空间的内积运算，大幅降低计算复杂度。


### 二、高斯核的定义
高斯核（也称为径向基函数核，RBF Kernel）的数学表达式为：  
$$ K(x, y) = \exp\left( -\frac{\|x - y\|^2}{2\sigma^2} \right) $$  
其中：  
- $|x - y\|^2$是低维空间中样本$x$和$y$的**欧氏距离平方**（衡量样本间的“物理距离”）；  
- $\sigma$是带宽参数（Bandwidth），控制核函数的“宽窄”；  


### 三、高斯核的核心原理
高斯核的原理可以从“相似度衡量”和“隐式高维映射”两个角度理解：  


#### 1. 本质是样本相似度的度量
高斯核的输出值$K(x, y)$本质上是**两个样本$x, y$的相似度**：  
- 当$x = y$时，欧氏距离$\|x - y\|^2 = 0$，此时$K(x, y) = \exp(0) = 1$（相似度最高）；  
- 当$x$和$y$距离越远时，$\|x - y\|^2$越大，指数部分越接近$-\infty$，$K(x, y)$越接近0（相似度越低）。  

因此，高斯核通过“距离越近，相似度越高”的逻辑，刻画了样本间的内在关联。


#### 2. 隐式映射到无限维空间
高斯核的关键特性是：**它对应一个从低维空间到无限维空间的隐式映射**。  

我们可以通过泰勒展开验证这一点：  
对于一维样本$x$，高斯核的映射函数$\phi(x)$可表示为无穷多个基函数的组合（例如$1, x, x^2, x^3, \dots$的加权形式）。这意味着，高斯核无需手动设计高维映射，就能自动将数据“嵌入”到无限维空间，从而理论上可以处理任意复杂的非线性关系。

为方便理解，做以下推导：  
已知高斯核（简化形式，省略$\sigma$参数）：  
$$ K(x_i, x_j) = \exp\left\{ -\lVert x_i - x_j \rVert^2 \right\} $$  


 
利用向量差的平方公式 $\lVert x_i - x_j \rVert^2 = x_i^2 + x_j^2 - 2x_i x_j$（假设 $x_i, x_j$ 为一维标量，多维可推广），代入核函数：  

$$ 
\begin{aligned}
K(x_i, x_j) &= \exp\left\{ -x_i^2 - x_j^2 + 2x_i x_j \right\} \\
 &= \exp\{-x_i^2\} \cdot \exp\{-x_j^2\} \cdot \exp\{2x_i x_j\} 
\end{aligned}
$$  


对$\boldsymbol{\exp\{2x_i x_j\}}$做泰勒展开:   
指数函数的泰勒展开式为 $\exp\{z\} = \sum_{n=0}^{\infty} \frac{z^n}{n!}$（$z \in \mathbb{R}$）。令 $z = 2x_i x_j$，则：  
$$ \exp\{2x_i x_j\} = \sum_{n=0}^{\infty} \frac{(2x_i x_j)^n}{n!} = \sum_{n=0}^{\infty} \frac{2^n}{n!} (x_i x_j)^n $$  


构造“隐式映射函数”$\boldsymbol{\phi(x)}$  ,观察展开式的结构，可将 $\exp\{-x_i^2\} \cdot \exp\{2x_i x_j\} \cdot \exp\{-x_j^2\}$ 重新组合为**两个向量的内积**。  

定义**无限维映射函数**：  
$$ \phi(x) = \exp\{-x^2\} \cdot \left( \sqrt{\frac{2^0}{0!}}, \sqrt{\frac{2^1}{1!}}x, \sqrt{\frac{2^2}{2!}}x^2, \dots, \sqrt{\frac{2^n}{n!}}x^n, \dots \right) $$  

此时，$\phi(x_i)$ 与 $\phi(x_j)$ 的内积为：  
$$ \phi(x_i) \cdot \phi(x_j) = \exp\{-x_i^2\} \exp\{-x_j^2\} \sum_{n=0}^{\infty} \frac{2^n}{n!} x_i^n x_j^n = \exp\{-x_i^2 - x_j^2 + 2x_i x_j\} $$  

这与高斯核的表达式完全一致，即：  
$$ K(x_i, x_j) = \phi(x_i) \cdot \phi(x_j) $$  


通过**泰勒展开 + 向量内积重组**，证明了高斯核可对应一个**无限维的隐式映射函数**$\phi(x)$：  
- 无需显式写出高维空间的全部维度，仅通过低维样本的指数运算，就能等价于高维空间的内积；  
- 映射函数$\phi(x)$的分量包含$x$的各阶幂次（$x^0, x^1, x^2, \dots$），系数由泰勒展开的余项决定。  


- 若 $x_i, x_j$ 是多维向量（如 $x = (x_1, x_2, \dots, x_d)$），推导逻辑类似，仅需将“一维幂次”替换为“多维多项式组合”（如 $x_1^a x_2^b \dots x_d^k$）；  
- 实际应用中，无需显式计算$\phi(x)$，直接用核函数公式即可完成高维内积的“隐式计算”（核技巧，Kernel Trick）。


#### 3. 带宽参数$\sigma$的作用
参数$\sigma$是高斯核的核心超参数，直接影响模型性能：  
- **$\sigma$较小时**：核函数曲线尖锐（“窄核”），只有与样本$x$非常近的样本$y$才会被认为有较高相似度（核值接近1）。此时模型容易过拟合（对噪声敏感，只关注局部细节）。  
- **$\sigma$较大时**：核函数曲线平缓（“宽核”），更多距离较远的样本会被认为有较高相似度。此时模型容易欠拟合（忽略局部差异，过度平滑数据）。  

形象地说，$\sigma$决定了模型“关注范围”的大小：$\sigma$越小，关注局部；$\sigma$越大，关注全局。

