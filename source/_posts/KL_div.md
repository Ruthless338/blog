---
title: KL散度
categories: 
    - 科研
tags: 
    - 概率
    - 机器学习
---

# KL散度的定义与公式

## 一、定义
KL散度（Kullback - Leibler Divergence），也被称为相对熵，是一种衡量两个概率分布差异的方法。它用于量化两个概率分布之间的距离，可以用来评估一个概率分布与另一个概率分布的相似程度。KL散度是非负的，当两个概率分布完全相同时，其值为0，否则为正数。

## 二、公式

对于离散概率分布，KL散度的公式为：
\[
D_{KL}(P \| Q) = \sum_{x \in \mathcal{X}} P(x) \log \frac{P(x)}{Q(x)}
\]
其中：
- \(P\) 和 \(Q\) 是定义在相同样本空间 \(\mathcal{X}\) 上的两个概率分布。
- \(P(x)\) 和 \(Q(x)\) 分别是概率分布 \(P\) 和 \(Q\) 在样本 \(x\) 处的概率值。
- \(\log\) 是自然对数。

对于连续概率分布，KL散度的公式为：
\[
D_{KL}(P \| Q) = \int_{-\infty}^{\infty} p(x) \log \frac{p(x)}{q(x)} dx = \int_{-\infty}^{\infty} p(x) \log p(x) - \log q(x) dx = E_{x-p}[\log p(x) - \log q(x)] 
\]
其中：
- \(p(x)\) 和 \(q(x)\) 分别是概率分布 \(P\) 和 \(Q\) 的概率密度函数。
- 积分范围是整个样本空间。

KL散度的值越大，表示两个概率分布之间的差异越大；值越小，表示两个概率分布越相似。