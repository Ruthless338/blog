---
title: Transformer架构学习
date: 2025-08-20
categories:
    - 科研
tags:
    - 大模型基础
    - 机器学习
---
Transformer 是一种基于自注意力机制（Self-Attention）的深度学习架构，最初由 Google 在 2017 年的论文《Attention is All You Need》中提出，主要用于自然语言处理（NLP）任务，后来被广泛应用于计算机视觉（如图像分类、分割）等领域。它的核心优势是能高效捕捉输入数据中的**长距离依赖关系**（如文本中上下文的关联、图像中不同区域的关联性），且并行计算能力远超传统的循环神经网络（RNN）。


### 一、Transformer 的核心思想  
Transformer 完全基于**注意力机制**，摒弃了 RNN 等架构的序列依赖（需按顺序处理输入），通过“全局建模+并行计算”实现对长距离特征的捕捉。其核心设计遵循“编码器-解码器”结构，但两者共享自注意力机制的核心逻辑。


### 二、Transformer 的整体架构  
![架构](/images/Transformer-1.png)


以原始 NLP 任务（如机器翻译）为例，Transformer 架构分为**编码器（Encoder）** 和**解码器（Decoder）** 两部分，整体结构如图所示（简化版）：  

```
输入序列 → 嵌入层（Embedding）+ 位置编码（Positional Encoding）→ 编码器层（N 个堆叠）→ 上下文向量  
                                                                 ↓  
目标序列 → 嵌入层 + 位置编码 → 解码器层（N 个堆叠）→ 输出层（预测结果）  
```  


#### 1. 嵌入层（Embedding）  
- **作用**：将离散输入（如文本中的单词、图像中的像素块）转换为连续的向量表示（称为“嵌入向量”）。  
- 例如：在 NLP 中，每个单词被映射为一个固定维度的向量（如 512 维）；在计算机视觉中，图像被分割为多个 patches（如 16×16 像素块），每个 patch 被映射为向量。  


#### 2. 位置编码（Positional Encoding）  
- **作用**：Transformer 本身没有时序感知能力，需通过位置编码向输入向量中注入位置信息，让模型知道输入元素的顺序（如文本中单词的先后、图像中 patch 的空间位置）。  
- **实现**：通常使用正弦/余弦函数生成位置编码，公式为：  
  - 对于位置 `pos` 和维度 `i`：  
    - 偶数维度：$PE_{pos, 2i} = \sin(pos / 10000^{2i/d_{\text{model}}})$  
    - 奇数维度：$PE_{pos, 2i+1} = \cos(pos / 10000^{2i/d_{\text{model}}})$  
  - 位置编码与嵌入向量相加，得到包含“内容+位置”的输入特征。  


#### 3. 编码器（Encoder）  
编码器由 **N 个相同的编码器层** 堆叠而成（原始论文中 N=6），每个编码器层包含两个子层：  

- **（1）多头自注意力（Multi-Head Self-Attention）**  
  - **核心逻辑**：让模型从不同角度（“头”）关注输入序列中不同元素的关联程度。  
  - **步骤**：  
    1. 对输入向量进行线性投影，得到三个矩阵：查询（Query, Q）、键（Key, K）、值（Value, V）。这里可以通俗的理解Q是“我在找什么”，K是“别人有什么”，V是“被关注者的实际信息”。 
    2. 计算“注意力分数”：通过 Q 与 K 的点积（$Q \cdot K^T$）衡量每个元素与其他元素的相关性，再通过 softmax 归一化得到注意力权重；  
    3. “多头”并行：将 Q、K、V 分割为多个子矩阵（如 8 个头），每个头独立计算注意力，最后拼接结果并线性投影，得到最终输出；  
    4. 公式简化：  
       $$
       \text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V
       $$  
       $(\sqrt{d_k}$ 为缩放因子，即矩阵K的维度，避免点积结果过大导致 softmax 梯度消失）  

  - **例**：在文本翻译中，“猫”可能与“抓”“老鼠”的注意力权重更高；在图像分割中，心肌区域可能与周围血管的注意力权重更高。  

- **（2）前馈神经网络（Feed Forward Network, FFN）**  
  - 对多头自注意力的输出进行非线性变换，每个位置的特征独立处理（并行计算），通常由两个线性层和一个 ReLU 激活函数组成：  
    $$
    \text{FFN}(x) = \max(0, xW_1 + b_1)W_2 + b_2
    $$  

- **其他细节**：  
  - 每个子层（多头注意力、FFN）后都有**残差连接**（输入 + 子层输出）和**层归一化（Layer Normalization）**，用于稳定训练。  


#### 4. 解码器（Decoder）  
解码器同样由 N 个相同的解码器层堆叠而成，每个解码器层包含三个子层：  

- **（1）掩码多头自注意力（Masked Multi-Head Self-Attention）**  
  - 与编码器的多头自注意力类似，但加入“掩码”（Mask）机制：在处理目标序列时，只能关注当前位置及之前的元素（避免提前看到未来信息，如翻译时不能提前看后面的单词）。  

- **（2）编码器-解码器注意力（Encoder-Decoder Attention）**  
  - 让解码器关注编码器输出的“上下文向量”：此时 Q 来自解码器的掩码注意力输出，K 和 V 来自编码器的最终输出，实现“目标序列”与“输入序列”的关联（如翻译时目标语言单词与源语言单词的对应）。  

- **（3）前馈神经网络（FFN）**  
  - 与编码器的 FFN 结构相同。  


#### 5. 输出层  
解码器的最终输出通过一个线性层映射到目标词汇表（或类别）的维度，再经 softmax 得到每个元素的预测概率（如翻译中每个位置的单词概率）。  


### 三、Transformer 在计算机视觉中的适配（以图像分割为例）  
原始 Transformer 为 NLP 设计，应用于图像任务时需做如下调整：  

1. **输入处理**：将图像分割为固定大小的 patch（如 16×16 像素），每个 patch 被展平为向量后作为“序列元素”，类比文本中的单词。  
2. **位置编码**：需注入空间位置信息（如 patch 在图像中的坐标），而非时序位置。  
3. **架构简化**：许多视觉任务（如分割）采用“仅编码器”或“编码器-解码器”的简化结构（如 U-Net 式的 Transformer，如 Swin-Unet、BRAU-Net++），通过编码器提取多尺度特征，解码器恢复空间分辨率。  
